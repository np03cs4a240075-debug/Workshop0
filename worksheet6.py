# -*- coding: utf-8 -*-
"""worksheet6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mKd57xSNr5fSESMMIXLmjf99BF_jIj9Z

1. Sigmoid (Logistic) Function
"""

import numpy as np

def logistic_function(x):
    return 1 / (1 + np.exp(-x))

def test_logistic_function():
    x_scalar = 0
    assert round(logistic_function(x_scalar), 3) == 0.5

    x_pos = 2
    assert round(logistic_function(x_pos), 3) == round(1/(1+np.exp(-2)), 3)

    x_neg = -3
    assert round(logistic_function(x_neg), 3) == round(1/(1+np.exp(3)), 3)

    x_array = np.array([0, 2, -3])
    expected = np.array([0.5, 0.881, 0.047])
    assert np.all(np.round(logistic_function(x_array), 3) == expected)

    print("All tests passed!")

test_logistic_function()

"""2. Log-Loss Function"""

def log_loss(y_true, y_pred):
    """
    Computes log loss for binary classification.
    """
    y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)
    loss = -(y_true * np.log(y_pred)) - ((1 - y_true) * np.log(1 - y_pred))
    return loss

def test_log_loss():
    assert np.isclose(log_loss(1, 1), 0.0)
    assert np.isclose(log_loss(0, 0), 0.0)

    assert np.isclose(log_loss(1, 0.8), -np.log(0.8))
    assert np.isclose(log_loss(0, 0.2), -np.log(0.8))

    print("All tests passed!")

test_log_loss()

"""3️⃣ Cost Function (Average Log-Loss)"""

def cost_function(y_true, y_pred):
    assert len(y_true) == len(y_pred)

    n = len(y_true)
    loss_vec = -(y_true * np.log(y_pred)) - ((1 - y_true) * np.log(1 - y_pred))
    cost = np.sum(loss_vec) / n
    return cost

def test_cost_function():
    y_true = np.array([1, 0, 1])
    y_pred = np.array([0.9, 0.1, 0.8])

    expected = (
        -np.log(0.9) -
        np.log(0.9) -
        np.log(0.8)
    ) / 3

    result = cost_function(y_true, y_pred)
    assert np.isclose(result, expected, atol=1e-6)

    print("Test passed!")

test_cost_function()

"""4️⃣ Cost Function with Parameters (Vectorized)"""

def costfunction_logreg(X, y, w, b):
    n, d = X.shape
    z = np.dot(X, w) + b
    y_pred = logistic_function(z)
    cost = cost_function(y, y_pred)
    return cost

"""5️⃣ Compute Gradient"""

def compute_gradient(X, y, w, b):
    n, d = X.shape

    z = np.dot(X, w) + b
    y_pred = logistic_function(z)

    grad_w = -(1/n) * np.dot(X.T, (y - y_pred))
    grad_b = -(1/n) * np.sum(y - y_pred)

    return grad_w, grad_b

"""6️⃣ Gradient Descent"""

def gradient_descent(X, y, w, b, alpha, n_iter, show_cost=False, show_params=True):

    cost_history = []
    params_history = []

    for i in range(n_iter):
        grad_w, grad_b = compute_gradient(X, y, w, b)

        w -= alpha * grad_w
        b -= alpha * grad_b

        cost = costfunction_logreg(X, y, w, b)

        cost_history.append(cost)
        params_history.append((w.copy(), b))

        if show_cost and (i % 100 == 0 or i == n_iter - 1):
            print(f"Iteration {i}: Cost = {cost:.6f}")

        if show_params and (i % 100 == 0 or i == n_iter - 1):
            print(f"Iteration {i}: w = {w}, b = {b:.6f}")

    return w, b, cost_history, params_history

"""7️⃣ Prediction Function"""

def prediction(X, w, b, threshold=0.5):
    z = np.dot(X, w) + b
    y_prob = logistic_function(z)
    y_pred = (y_prob >= threshold).astype(int)
    return y_pred

"""8️⃣ Evaluation (Confusion Matrix, Precision, Recall, F1)"""

def evaluate_classification(y_true, y_pred):
    TP = np.sum((y_true == 1) & (y_pred == 1))
    TN = np.sum((y_true == 0) & (y_pred == 0))
    FP = np.sum((y_true == 0) & (y_pred == 1))
    FN = np.sum((y_true == 1) & (y_pred == 0))

    confusion_matrix = np.array([[TN, FP],
                                  [FN, TP]])

    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0
    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0
    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0

    return confusion_matrix, precision, recall, f1_score

"""CELL 9: Cost vs Iteration Plot (Visualization)"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv"
columns = [
    'Pregnancies','Glucose','BloodPressure','SkinThickness',
    'Insulin','BMI','DiabetesPedigreeFunction','Age','Outcome'
]

df = pd.read_csv(url, names=columns)

cols = ['Glucose','BloodPressure','SkinThickness','Insulin','BMI']
df[cols] = df[cols].replace(0, np.nan)
df.fillna(df.median(), inplace=True)

X = df.drop(columns=['Outcome']).values
y = df['Outcome'].values

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

w = np.zeros(X_train.shape[1])
b = 0.0

w, b, cost_history, _ = gradient_descent(
    X_train, y_train, w, b, alpha=0.1, n_iter=1000
)

plt.plot(cost_history)
plt.xlabel("Iteration")
plt.ylabel("Cost")
plt.title("Cost vs Iteration")
plt.show()

"""10. Final evaluation"""

y_test_pred = prediction(X_test, w, b)

confusion_matrix, precision, recall, f1_score = evaluate_classification(
    y_test, y_test_pred
)

print("Confusion Matrix:\n", confusion_matrix)
print("Precision:", round(precision, 2))
print("Recall:", round(recall, 2))
print("F1 Score:", round(f1_score, 2))
print("Accuracy:", round(np.mean(y_test_pred == y_test) * 100, 2), "%")

