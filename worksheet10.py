# -*- coding: utf-8 -*-
"""worksheet10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y-DyaWzuAA6bhwB-fpeJbxuOlwuWdan8

EXERCISE 1: Naive Bayes
Sentiment Analysis – IMDB
"""

#1. Import required libraries
import pandas as pd
import numpy as np
import re
import nltk

from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score

#Load the IMDB Dataset
df = pd.read_csv("IMDB Dataset.csv")

print("Part 1 – Q1: Dataset Loaded")
print("Dataset Shape:", df.shape)
df.head()

#Clean & Encode Target Variable

# Remove extra spaces, convert to lowercase
df['sentiment'] = df['sentiment'].str.lower().str.strip()

# Map to 0/1
df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})

# Drop rows with NaN just in case
df = df.dropna(subset=['sentiment'])

# Confirm no NaN remains
print("NaN in sentiment column:", df['sentiment'].isnull().sum())

nltk.download('stopwords')

ps = PorterStemmer()
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    # a. lowercase
    text = text.lower()

    # b. remove non-alphabetic chars
    text = re.sub('[^a-z]', ' ', text)

    # c. tokenize + remove stopwords
    words = text.split()
    words = [w for w in words if w not in stop_words]

    # d. stemming
    words = [ps.stem(w) for w in words]

    return " ".join(words)

#Apply preprocessing to reviews
df['clean_review'] = df['review'].apply(preprocess_text)

print("Part 1 – Q1: Text Preprocessing Done")
print("\nOriginal Review:\n", df['review'][0])
print("\nCleaned Review:\n", df['clean_review'][0])

#Part 1 – Question 2:
#Train-Test Split (80% / 20%)
X = df['clean_review']
y = df['sentiment']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("Part 1 – Q2: Train-Test Split Done")
print("Training samples:", X_train.shape)
print("Testing samples:", X_test.shape)

#Question 3:
#Naive Bayes Classifier

#(a) Bag-of-Words using CountVectorizer
cv = CountVectorizer(max_features=5000)

X_train_bow = cv.fit_transform(X_train)
X_test_bow = cv.transform(X_test)

print("Part 1 – Q3(a): Bag-of-Words Created")
print("BoW Train Shape:", X_train_bow.shape)
print("BoW Test Shape:", X_test_bow.shape)

#(b) Train Naive Bayes Model
nb_model = MultinomialNB()
nb_model.fit(X_train_bow, y_train)

print("Part 1 – Q3(b): Naive Bayes Model Trained SUCCESSFULLY")

"""Part 2: Model Evaluation"""

#Part 2 – Question 1
y_pred = nb_model.predict(X_test_bow)
y_prob = nb_model.predict_proba(X_test_bow)[:, 1]

print("Part 2 – Predictions Generated")
print("Sample Predictions:", y_pred[:10])
print("Actual Labels:", y_test.values[:10])

#a) Accuracy
print("\nPart 2 – Q1(a): Accuracy")
print(accuracy_score(y_test, y_pred))

#b) Precision, Recall, F1-score
print("\nPart 2 – Q1(b): Precision, Recall, F1-score")
print(classification_report(y_test, y_pred))

#c) Confusion Matrix
print("\nPart 2 – Q1(c): Confusion Matrix")
print(confusion_matrix(y_test, y_pred))

#d) ROC-AUC Score
print("\nPart 2 – Q1(d): ROC-AUC Score")
print(roc_auc_score(y_test, y_prob))

"""#QUESTION 2: FEATURE SELECTION (WRAPPER METHOD)"""

#Part 1: Data Loading & EDA
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFE
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

print("Part 1 – Q1: Libraries Imported")

#Part 1 – Question 2: Load
# Replace file path with the downloaded CSV
# Load Kaggle WPBC dataset
import pandas as pd


# Load dataset
df = pd.read_csv("breast-cancer-wisconsin-prognostic-data-set.csv")


# View first 5 rows
df.head()

#Part 1 – Question 3: EDA
# Check columns
print("\nColumns:", df_bc.columns.tolist())

print("Summary Statistics of Dataset:")
df.describe()

#Part 1 – Q3(b): Check Missing Values
print("Missing values in each column:")
df.isnull().sum()

#Part 1 – Q3(c): Handle Missing Values
# Drop fully empty column
df.drop(columns=['Unnamed: 32'], inplace=True)

print("Column 'Unnamed: 32' removed ✅")
print("New shape:", df.shape)

#Part 1 – Q4: Encode & Train-Test Split
from sklearn.model_selection import train_test_split

# Encode target
df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0})

X = df.drop(columns=['id', 'diagnosis'])
y = df['diagnosis']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("Training set size:", X_train.shape)
print("Testing set size:", X_test.shape)

#Part 2 – Q1: RFE with Logistic Regression
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFE

log_reg = LogisticRegression(max_iter=5000)

rfe = RFE(estimator=log_reg, n_features_to_select=5)
rfe.fit(X_train, y_train)

selected_features = X.columns[rfe.support_]

print("Top 5 Selected Features:")
for f in selected_features:
    print("✔", f)

#Feature Ranking Output
ranking_df = pd.DataFrame({
    'Feature': X.columns,
    'Ranking': rfe.ranking_
}).sort_values('Ranking')

ranking_df

#Feature Ranking Visualization
import matplotlib.pyplot as plt
plt.figure(figsize=(10,6))
plt.barh(ranking_df['Feature'], ranking_df['Ranking'])
plt.xlabel("RFE Ranking (Lower = More Important)")
plt.title("Feature Importance using RFE")
plt.gca().invert_yaxis()
plt.show()

#Part 2 – Q2: Train Model using Selected Features
X_train_rfe = X_train[selected_features]
X_test_rfe = X_test[selected_features]

log_reg.fit(X_train_rfe, y_train)

print("Model trained using selected features ")

#part 2- question 3
import pandas as pd
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score
)

#Step 2: Make predictions on testing set
# Predictions
y_pred = log_reg.predict(X_test_rfe)
y_prob = log_reg.predict_proba(X_test_rfe)[:, 1]

print("Predictions completed successfully ")

#Accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
#Precision
precision = precision_score(y_test, y_pred)
print("Precision:", precision)
#Recall
recall = recall_score(y_test, y_pred)
print("Recall:", recall)
#F1-Score
f1 = f1_score(y_test, y_pred)
print("F1-Score:", f1)
#ROC-AUC
roc_auc = roc_auc_score(y_test, y_prob)
print("ROC-AUC:", roc_auc)

metrics_part3_q1 = {
    "Accuracy": accuracy,
    "Precision": precision,
    "Recall": recall,
    "F1-Score": f1,
    "ROC-AUC": roc_auc
}

pd.DataFrame(metrics_part3_q1, index=["Selected Features Model"])

# PART 3 – Q2
# Model trained using ALL features

# Train model with all features
log_reg.fit(X_train, y_train)

# Predictions
y_pred_all = log_reg.predict(X_test)
y_prob_all = log_reg.predict_proba(X_test)[:, 1]

# Calculate metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

all_feature_results = {
    "Accuracy": accuracy_score(y_test, y_pred_all),
    "Precision": precision_score(y_test, y_pred_all),
    "Recall": recall_score(y_test, y_pred_all),
    "F1-Score": f1_score(y_test, y_pred_all),
    "ROC-AUC": roc_auc_score(y_test, y_prob_all)
}

print("All Features Model Metrics calculated ")
all_feature_results
# Part 3 – Q2: Comparison Table
comparison = pd.DataFrame(
    [all_feature_results, rfe_results],
    index=["All Features", "Top 5 Features"]
)

comparison

#Part 4 – Q1: Top 3 vs Top 7 Features
results = {}

for n in [3, 5, 7]:
    rfe_n = RFE(log_reg, n_features_to_select=n)
    rfe_n.fit(X_train, y_train)

    feats = X.columns[rfe_n.support_]

    log_reg.fit(X_train[feats], y_train)
    y_pred = log_reg.predict(X_test[feats])
    y_prob = log_reg.predict_proba(X_test[feats])[:,1]

    results[f"Top {n} Features"] = evaluate_model(y_test, y_pred, y_prob)

pd.DataFrame(results).T

"""Part 4 – Q2: Discussion

Feature selection using RFE helps reduce dimensionality while maintaining performance.
Using top 5 features provided nearly the same accuracy and ROC-AUC as using all features.
Too few features (top 3) slightly reduced performance, while increasing to top 7 did not significantly improve results.
Hence, optimal feature selection balances simplicity and predictive performance.
"""

