# -*- coding: utf-8 -*-
"""worksheet4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IvssGhE45CWWFon2YK_lBuF75KwePflp

Problem-1
"""

# 1.1 Load dataset and EDA
import pandas as pd
import numpy as np

df = pd.read_csv("diabetes_.csv")
print("First 5 rows:")
print(df.head())
print("\nInfo:")
print(df.info())
print("\nDescribe:")
print(df.describe())
print("\nZero counts per column (0 may indicate missing in some cols):")
print((df == 0).sum())
print("\nNull counts:")
print(df.isnull().sum())

# 1.2 Handle missing values
cols_with_zero_missing = ["Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI"]
for col in cols_with_zero_missing:
    if col in df.columns:
        nonzero_median = df.loc[df[col] != 0, col].median()
        df.loc[df[col] == 0, col] = nonzero_median
print("After imputation, zero counts:")
print((df == 0).sum())

# 1.3 Feature matrix and label + custom train-test split
possible_labels = ["Outcome", "Diabetes", "Class", "target", "Label"]
label_col = next((c for c in possible_labels if c in df.columns), df.columns[-1])
print("Using label column:", label_col)

X = df.drop(columns=[label_col]).values.astype(float)
y = df[label_col].values.astype(int)

def train_test_split_scratch(X, y, test_size=0.3, random_seed=42):
    np.random.seed(random_seed)
    idx = np.arange(X.shape[0])
    np.random.shuffle(idx)
    split = int(len(idx) * (1 - test_size))
    train_idx = idx[:split]
    test_idx = idx[split:]
    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]

X_train, X_test, y_train, y_test = train_test_split_scratch(X, y, test_size=0.3, random_seed=42)
print("Shapes:", X_train.shape, X_test.shape, y_train.shape, y_test.shape)

# 1.4 Euclidean and KNN functions
import math

def euclidean_distance(a, b):
    a = np.asarray(a); b = np.asarray(b)
    return math.sqrt(np.sum((a - b) ** 2))

def knn_predict_single(query, X_train, y_train, k=3):
    distances = np.array([euclidean_distance(query, x) for x in X_train])
    nearest_idx = np.argsort(distances)[:k]
    nearest_labels = y_train[nearest_idx]
    return np.bincount(nearest_labels).argmax()

def knn_predict(X_test, X_train, y_train, k=3):
    return np.array([knn_predict_single(q, X_train, y_train, k) for q in X_test])

def compute_accuracy(y_true, y_pred):
    return np.sum(y_true == y_pred) / len(y_true) * 100.0

# 1.5 Run and evaluate (original data)
k = 3
preds = knn_predict(X_test, X_train, y_train, k=k)
acc = compute_accuracy(y_test, preds)
print(f"Original data: k={k}, Accuracy = {acc:.2f}%")
print("Sample predictions:", preds[:10])
print("Sample actuals    :", y_test[:10])

"""Problem 2 — Scale features and compare"""

# 2.1 Scale features (standardization)
X_mean = X_train.mean(axis=0)
X_std = X_train.std(axis=0)
X_std_safe = np.where(X_std == 0, 1, X_std)  # avoid div by 0

X_train_scaled = (X_train - X_mean) / X_std_safe
X_test_scaled = (X_test - X_mean) / X_std_safe

# quick shape check
print("Scaled shapes:", X_train_scaled.shape, X_test_scaled.shape)

# 2.2 Run KNN on scaled data
k = 3
preds_scaled = knn_predict(X_test_scaled, X_train_scaled, y_train, k=k)
acc_scaled = compute_accuracy(y_test, preds_scaled)
print(f"Scaled data: k={k}, Accuracy = {acc_scaled:.2f}%")

print("\nComparison:")
print(f"Original acc (k={k}): {acc:.2f}%")
print(f"Scaled   acc (k={k}): {acc_scaled:.2f}%")

"""Problem 3 — Experiment k = 1..15 (accuracy & time) for original and scaled"""

# 3.1 Experiment k from 1 to 15, record accuracy and time
import time
results = []
k_values = range(1, 16)

for k in k_values:
    t0 = time.perf_counter()
    p_orig = knn_predict(X_test, X_train, y_train, k=k)
    t1 = time.perf_counter()
    acc_orig = compute_accuracy(y_test, p_orig)
    time_orig = t1 - t0

    t0 = time.perf_counter()
    p_scaled = knn_predict(X_test_scaled, X_train_scaled, y_train, k=k)
    t1 = time.perf_counter()
    acc_scaled = compute_accuracy(y_test, p_scaled)
    time_scaled = t1 - t0

    results.append({
        "k": k,
        "accuracy_original": acc_orig,
        "time_original_sec": time_orig,
        "accuracy_scaled": acc_scaled,
        "time_scaled_sec": time_scaled
    })
    print(f"k={k}: orig_acc={acc_orig:.2f}%, orig_time={time_orig:.4f}s | scaled_acc={acc_scaled:.2f}%, scaled_time={time_scaled:.4f}s")

import os

# Check directory exists or not
print("Does /mnt/data exist? ->", os.path.isdir("/mnt/data"))

# If not exist, create it
if not os.path.exists("/mnt/data"):
    os.makedirs("/mnt/data")
    print("Created directory /mnt/data")

# Now save results safely
results_df = pd.DataFrame(results)
results_df.to_csv("/mnt/data/knn_experiment_results.csv", index=False)

print("File saved successfully to /mnt/data/knn_experiment_results.csv")

# 3.2 Visualization of accuracy and time

import matplotlib.pyplot as plt

# Extract values from results dictionary
k_vals = [r["k"] for r in results]
acc_orig = [r["accuracy_original"] for r in results]
acc_scaled = [r["accuracy_scaled"] for r in results]
time_orig = [r["time_original_sec"] for r in results]
time_scaled = [r["time_scaled_sec"] for r in results]

# ---- Accuracy Plot ----
plt.figure(figsize=(10,5))
plt.plot(k_vals, acc_orig, marker='o', label="Original Accuracy")
plt.plot(k_vals, acc_scaled, marker='o', label="Scaled Accuracy")
plt.xlabel("k value")
plt.ylabel("Accuracy (%)")
plt.title("KNN Accuracy vs k (Original vs Scaled)")
plt.grid(True)
plt.legend()
plt.show()

# ---- Time Plot ----
plt.figure(figsize=(10,5))
plt.plot(k_vals, time_orig, marker='o', label="Original Time (sec)")
plt.plot(k_vals, time_scaled, marker='o', label="Scaled Time (sec)")
plt.xlabel("k value")
plt.ylabel("Prediction Time (seconds)")
plt.title("Prediction Time vs k (Original vs Scaled)")
plt.grid(True)
plt.legend()
plt.show()

# 3.3 Summary / Discussion based on results

# Find best k for original
best_orig = max(results, key=lambda x: x["accuracy_original"])
# Find best k for scaled
best_scaled = max(results, key=lambda x: x["accuracy_scaled"])

print("--------- FINAL SUMMARY (3.3) ---------")
print(f"Best k (Original Dataset): k={best_orig['k']} with accuracy={best_orig['accuracy_original']:.2f}%")
print(f"Best k (Scaled Dataset):   k={best_scaled['k']} with accuracy={best_scaled['accuracy_scaled']:.2f}%")

print("\nInterpretation:")
print("Scaling generally improved consistency and gave smoother accuracy values.")
print("Original dataset achieved the highest single accuracy (k=14),")
print("while scaled dataset performed best at medium k values (k=5).")
print("Prediction time remained similar for original and scaled datasets.")
print("This shows scaling improves accuracy but does not significantly affect computation time.")
print("----------------------------------------")

