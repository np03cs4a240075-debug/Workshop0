# -*- coding: utf-8 -*-
"""worksheet5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dUNCWiJZ9UClE6rB88l1Fu7J89B4l3gt
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

"""To-Do 1: Read and Observe the Dataset"""

data = pd.read_csv('student.csv')


print("Top 5 rows:\n", data.head())
print("\nBottom 5 rows:\n", data.tail())
print("\nDataset Info:\n")
data.info()
print("\nDataset Description:\n", data.describe())


# Split Features (X) and Label (Y)
X = data[['Math', 'Reading']].values
Y = data['Writing'].values

"""To-Do 2: Create Matrices (No Bias Term)"""

# Y = W^T X
# X shape: (n_samples, n_features)
# W shape: (n_features, )

"""TO‑DO 3: Train‑Test Split"""

X_train, X_test, Y_train, Y_test = train_test_split(
X, Y, test_size=0.2, random_state=42
)
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("Y_train shape:", Y_train.shape)
print("Y_test shape:", Y_test.shape)

"""To-Do 4: Cost Function (MSE)"""

def cost_function(X, Y, W):
    """
    Mean Squared Error Cost Function
    """
    n = len(Y)
    Y_pred = np.dot(X, W)
    cost = (1 / (2 * n)) * np.sum((Y_pred - Y) ** 2)
    return cost

"""To-Do 5: Test Cost Function"""

X_test_case = np.array([[1, 2], [3, 4], [5, 6]])
Y_test_case = np.array([3, 7, 11])
W_test_case = np.array([1, 1])

cost = cost_function(X_test_case, Y_test_case, W_test_case)

if cost == 0:
    print("Proceed Further")
else:
    print("Something went wrong")

print("Cost function output:", cost)

"""To-Do 6: Gradient Descent Implementation"""

def gradient_descent(X, Y, W, alpha, iterations):
    """
    Perform gradient descent to optimize linear regression parameters
    """
    cost_history = [0] * iterations
    m = len(Y)

    for i in range(iterations):

        # Step 1: Prediction
        Y_pred = np.dot(X, W)

        # Step 2: Loss
        loss = Y_pred - Y

        # Step 3: Gradient
        dw = (1 / m) * np.dot(X.T, loss)

        # Step 4: Update Weights
        W = W - alpha * dw

        # Step 5: Cost
        cost_history[i] = cost_function(X, Y, W)

    return W, cost_history

"""To-Do 7: Test Gradient Descent"""

np.random.seed(0)

X = np.random.rand(100, 3)
Y = np.random.rand(100)
W = np.random.rand(3)

alpha = 0.01
iterations = 1000

final_params, cost_history = gradient_descent(X, Y, W, alpha, iterations)

print("Final Parameters:", final_params)
print("First 5 cost values:", cost_history[:5])

"""To-Do 8: RMSE Function"""

def rmse(Y, Y_pred):
  error = np.sqrt(np.mean((Y - Y_pred) ** 2))
  return error

"""To-Do 9: R-Squared Function"""

def r2(Y, Y_pred):
  mean_y = np.mean(Y)
  ss_tot = np.sum((Y - mean_y) ** 2)
  ss_res = np.sum((Y - Y_pred) ** 2)
  r2_score = 1 - (ss_res / ss_tot)
  return r2_score

"""To-Do 10: Main Function"""

def main():
  data = pd.read_csv('student.csv')


  X = data[['Math', 'Reading']].values
  Y = data['Writing'].values


  X_train, X_test, Y_train, Y_test = train_test_split(
  X, Y, test_size=0.2, random_state=42
)


  W = np.zeros(X_train.shape[1])
  alpha = 0.00001
  iterations = 1000


  W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)


  Y_pred = np.dot(X_test, W_optimal)


  model_rmse = rmse(Y_test, Y_pred)
  model_r2 = r2(Y_test, Y_pred)


  print("Final Weights:", W_optimal)
  print("Cost History (First 10):", cost_history[:10])
  print("RMSE:", model_rmse)
  print("R2 Score:", model_r2)

if __name__ == '__main__':
  main()

"""To do- 11

The performance of the linear regression model is acceptable. The model produces a low RMSE value and a reasonably high R-squared value, which shows that the model predicts writing marks accurately using math and reading marks. The cost function decreases over iterations, indicating proper learning.

When using a very small learning rate, the model learns slowly. With a very large learning rate, the model becomes unstable and may not converge. A moderate learning rate provides stable convergence and good performance.

Overall, the model neither overfits nor underfits the data and performs well on the test dataset.
"""

